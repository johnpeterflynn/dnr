{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)\n",
    "PATH = \".\"\n",
    "DATA_DIR = os.path.join(PATH, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models/rendernet.py\n",
    "%run data_loaders/scannet_render_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model using its best weights unless otherwise specified\n",
    "def load_trained_model(train_id, checkpoint_name='model_best'):\n",
    "    model_path = os.path.join(PATH, 'saved/models/DNR', train_id)\n",
    "\n",
    "    # Load config file\n",
    "    config_file = os.path.join(model_path, \"config.json\")\n",
    "    if config_file:\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "    # Load model weights\n",
    "    checkpoint_path = os.path.join(model_path, checkpoint_name) + '.pth'\n",
    "    \n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        pose_files = glob.glob(os.path.join(model_path, '*.pth'))\n",
    "        \n",
    "        def extract_epoch(f):\n",
    "            s = re.findall(\"checkpoint-epoch(\\d+)\", f)\n",
    "            return (int(s[0]) if s else -1,f)\n",
    "        \n",
    "        checkpoint_path = max(pose_files,key=extract_epoch)\n",
    "    \n",
    "    print('loaded:', checkpoint_path)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "    # Handling loading of models from different versions will be tricky.\n",
    "    # Eventually might need to check models out from git repo. For now,\n",
    "    # use these fixes.\n",
    "    if 'mipmap_levels' in config['arch']['args']:\n",
    "        mipmap_levels = config['arch']['args']['mipmap_levels']\n",
    "        zero_other_mipmaps = False\n",
    "        print('contains mipmap:', mipmap_levels)\n",
    "    else:\n",
    "        mipmap_levels = 1\n",
    "        checkpoint['state_dict']['neural_texture.mipmap.0'] = checkpoint['state_dict'].pop('neural_texture.texture')\n",
    "        zero_other_mipmaps = True\n",
    "        print('Warning: {} weights from old model version with only one mipmap layer'.format(train_id))\n",
    "    \n",
    "    # Ahh! We have to hack the weights since ParameterList isn't accepted by TorchScript\n",
    "    if 'neural_texture.mipmap.0' in checkpoint['state_dict']:\n",
    "        print('Warning: {} mipmap from old model version that used ParameterList or a single layer'.format(train_id))\n",
    "        for i in range(mipmap_levels):\n",
    "            checkpoint['state_dict']['neural_texture.mipmap_{}'.format(i)] = checkpoint['state_dict'].pop('neural_texture.mipmap.{}'.format(i))\n",
    "    \n",
    "    # Load model with parameters from config file\n",
    "    model = RenderNet(config['arch']['args']['texture_size'],\n",
    "                     config['arch']['args']['texture_depth'],\n",
    "                     mipmap_levels)\n",
    "    \n",
    "    # TODO: WARNING: Leaving some mipmap layer weights unassigned might lead to erroneous\n",
    "    #  results (maybe they're not set to zero by default)\n",
    "    # Assign model weights and set to eval (not train) mode\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=(not zero_other_mipmaps))\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all models by train id\n",
    "def load_trained_models(train_ids):\n",
    "    models = {}\n",
    "    for train_id in train_ids:\n",
    "        models[train_id] = load_trained_model(train_id)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cereate a libtorch script file containing the model that can be loaded into C++\n",
    "def create_libtorch_script(model, train_id, checkpoint_name='model_best'):\n",
    "    sm = torch.jit.script(model)\n",
    "    model_script_name = 'DNR-{}-{}_model.pt'.format(train_id, checkpoint_name)\n",
    "    model_script_path = os.path.join(PATH, 'libtorch-models', model_script_name)\n",
    "    sm.save(model_script_path)\n",
    "    print(model_script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a model prediction\n",
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input)#, training=True)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    \n",
    "    _, h, w, c = test_input.shape\n",
    "    test_input_color = torch.zeros((h, w, 3))#, dtype=type(test_input))\n",
    "    test_input_color = test_input[:,:,:, 0]\n",
    "    tar = tar.permute(0, 2, 3, 1)\n",
    "    prediction = prediction.detach().permute(0, 2, 3, 1)\n",
    "\n",
    "    display_list = [test_input_color[0].numpy(), tar[0].numpy(), prediction[0].numpy()]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison(display_images, title, title_color):\n",
    "    for i, image in enumerate(display_images):\n",
    "        display_images[i] = image.permute(0, 2, 3, 1)\n",
    "        \n",
    "    # Should assert that rows * cols == len(title) == len(display_images)\n",
    "    img_per_row = 2 # 3\n",
    "    rows, cols = np.ceil(len(display_images) / img_per_row), np.min([len(display_images), img_per_row])\n",
    "    plt.figure(figsize=(35 * cols,30 * rows))\n",
    "    for i in range(len(display_images)):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.title(title[i], color=title_color[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_images[i][0].numpy() * 0.5 + 0.5)\n",
    "        #plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mae(display_images, target, title):\n",
    "    diffs = []\n",
    "    target = target.permute(0, 2, 3, 1)\n",
    "    for i, image in enumerate(display_images):\n",
    "        display_images[i] = image.permute(0, 2, 3, 1)\n",
    "        diff = torch.sum(torch.abs(target - display_images[i]), axis=3)\n",
    "        diffs.append(diff)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Should assert that rows * cols == len(title) == len(display_images)\n",
    "    rows, cols = len(display_images), 2\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.title(\"Ground truth\")\n",
    "    plt.imshow(target[0].numpy() * 0.5 + 0.5)\n",
    "    plt.figure(figsize=(8 * cols,8 * rows))\n",
    "    for i in range(0, len(display_images)):\n",
    "        plt.subplot(rows, cols, 2*i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_images[i][0].numpy() * 0.5 + 0.5)\n",
    "        \n",
    "        plt.subplot(rows, cols, 2*i+2)\n",
    "        plt.title('MAE. Mean: {}'.format(torch.mean(diffs[i])))\n",
    "        plt.imshow(1.0 - (diffs[i][0] / 2).numpy(), cmap='gray')\n",
    "        #plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_texture(neural_texture, select=None, disp_channels=None):\n",
    "    mipmap = [neural_texture.mipmap_0,\n",
    "              neural_texture.mipmap_1,\n",
    "              neural_texture.mipmap_2,\n",
    "              neural_texture.mipmap_3]\n",
    "    \n",
    "    _, _, _, size = mipmap[0].shape\n",
    "    sample = 0\n",
    "    for i, texture in enumerate(mipmap):\n",
    "        if select is not None and i != select:\n",
    "            continue\n",
    "        sample += F.interpolate(texture.detach(), size=size, mode='bilinear', align_corners=False)\n",
    "    \n",
    "    sample = sample[0,:,:,:]\n",
    "    sample = sample.permute(1, 2, 0)\n",
    "    \n",
    "    height, width, channels = sample.shape\n",
    "    \n",
    "    if disp_channels is None:\n",
    "        disp_channels = channels\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(disp_channels):\n",
    "        plt.title('Channel {}'.format(i))\n",
    "        plt.subplot(np.ceil(channels / 4), 4, i+1)\n",
    "        plt.imshow(sample[:, :, i].numpy() * 0.5 + 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_color_texture(neural_texture):\n",
    "    mipmap = [neural_texture.mipmap_0,\n",
    "              neural_texture.mipmap_1,\n",
    "              neural_texture.mipmap_2,\n",
    "              neural_texture.mipmap_3]\n",
    "    \n",
    "    _, _, _, size = mipmap[0].shape\n",
    "    sample = 0\n",
    "    for i, texture in enumerate(mipmap):\n",
    "        sample += F.interpolate(texture[:,0:3,:,:].detach(), size=size, mode='bilinear', align_corners=False)\n",
    "    \n",
    "    sample = sample[0,:,:,:]\n",
    "    sample = sample.permute(1, 2, 0)\n",
    "    \n",
    "    rescale = 2 / (torch.min(sample) - torch.max(sample))\n",
    "    sample = sample * rescale\n",
    "    print(torch.min(sample), torch.max(sample)) \n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.title('Color')\n",
    "    #plt.subplot(1, 1, 1)\n",
    "    plt.imshow(sample.numpy() * 0.5 + 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Execute code below --##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List train ids here #\n",
    "#train_ids = ['0626_000812', '0710_005202', '0714_232542', '0716_003726', '0716_152355', '0717_005959']\n",
    "#train_ids = ['0714_232542', '0716_003726', '0716_152355', '0717_005959']\n",
    "#titles = ['Single NT Layer, LowQ UV Coords, 1024', 'Single NT Layer, LowQ UV Coords, 1024, Stronger Data Aug', 'NT Hierarchies, LowQ UV Coords, 1024', 'NT Hierarchies, HighQ Coords, 1024', 'NT Hierarchies, HighQ UV Coords, 2048', 'Filtered Dataset', 'Colorized Dataset']\n",
    "#train_ids = ['0710_005202', '0711_085844', '0714_232542', '0716_152355', '0717_005959', '0721_015234', '0722_010614']\n",
    "\n",
    "titles = ['High-res Model', 'Low-res Model']\n",
    "train_ids = ['0724_011558', '0724_122900']\n",
    "uv_folder_names = ['uv/scene_highres_vertexuv_cubed_proj', 'uv/scene_lowres_vertexuv_cubed_proj']\n",
    "models = load_trained_models(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_all_textures():\n",
    "    num_layers = 4\n",
    "    for train_id in train_ids:\n",
    "        print('==================== \\/ == Model', train_id, '== \\/ ====================')\n",
    "        visualize_color_texture(models[train_id].neural_texture)\n",
    "        for layer in range(num_layers):\n",
    "            print('--> Layer:', layer)\n",
    "            visualize_texture(models[train_id].neural_texture, select=layer, disp_channels=4)\n",
    "\n",
    "# Run\n",
    "visualize_all_textures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a single validation input, ground truth and preducted sample from the first model #\n",
    "def display_uv_ground_truth_predicted(uv_folder_name, filter_file):\n",
    "    loader = UVDataLoader('data/scene0000_00', uv_folder_name, 'color', filter_file, 1, shuffle=False, skip=6, compressed_input=True).split_validation()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        for train_id in train_ids:\n",
    "            print('Train ID:', train_id)\n",
    "            model = models[train_id]  \n",
    "            generate_images(model, data, target)\n",
    "        break\n",
    "\n",
    "# Run over all models for a specific dataset\n",
    "display_uv_ground_truth_predicted(uv_folder_names[0], filter_file='filters/keypoint_blur_1.2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show a validation sample prediction for each model #\n",
    "def display_prediction_mae(frame_index, filter_file, size):\n",
    "    display_images = []\n",
    "    for i, train_id in enumerate(train_ids):\n",
    "        # Load from the validatiom dataset\n",
    "        loader = UVDataLoader('data', uv_folder_names[i], 'color', filter_file, 1, shuffle=False, skip=6, compressed_input=True, size=size)\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            if batch_idx < frame_index:\n",
    "                continue\n",
    "\n",
    "            # Get the trained model\n",
    "            model = models[train_id]\n",
    "\n",
    "            # Make a prediction using the model\n",
    "            prediction = model(data)\n",
    "            prediction = prediction.detach()\n",
    "            display_images.append(prediction)\n",
    "            break\n",
    "\n",
    "    # Plot results\n",
    "    generate_mae(display_images, target.detach(), titles)\n",
    "\n",
    "# Run\n",
    "display_prediction_mae(frame_index=20, filter_file='filters/keypoint_blur_1.2.txt', size=(968, 1296)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show a validation sample prediction for each model #\n",
    "\n",
    "display_images = []\n",
    "# Load from the validatiom dataset\n",
    "loader = UVDataLoader('data', 'data/keep_1.2.txt', 1, True, 6).split_validation()\n",
    "for batch_idx, (data, target) in enumerate(loader):\n",
    "    print('data size:', data.size)\n",
    "    # Add target image used to generate predictions\n",
    "    display_images.append(target)\n",
    "\n",
    "    # Add predictions\n",
    "    for train_id in train_ids:\n",
    "        # Get the trained model\n",
    "        model = models[train_id]\n",
    "        \n",
    "        # Make a prediction using the model\n",
    "        prediction = model(data)\n",
    "        prediction = prediction.detach()\n",
    "        display_images.append(prediction)\n",
    "    break\n",
    "\n",
    "# Plot results\n",
    "title = ['Ground Truth', 'Prediction Exp 1: 521 train, 104 val',\n",
    "         'Prediction Exp 1: 1042 train, 208 val',\n",
    "         'Prediction Exp 1: 2083 train, 417 val', '', '', '']\n",
    "title_color = ['black', 'magenta', 'green', 'blue', 'blue', 'blue', 'blue']\n",
    "\n",
    "generate_comparison(display_images, title, title_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a libtorch script file from model #\n",
    "def generate_torchscript_model(train_id)\n",
    "    print(train_id)\n",
    "    script_model = load_trained_model(train_id)\n",
    "    create_libtorch_script(script_model, train_id)\n",
    "    \n",
    "# Run\n",
    "generate_torchscript_model(train_id='0717_005959')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
